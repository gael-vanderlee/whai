---
alwaysApply: true
---

# Technical Implementation

## Overview

whai is a modular Python CLI tool. It uses a persistent shell subprocess to run user-approved commands and integrates with a large language model for terminal assistance. The focus is on explicit user control, modularity, and safety by design.

## Main Architecture and Execution Flow

1. Launch: User runs `whai "question or instruction"`
2. Configuration loading (handled by `config.py`): Reads `config.toml` and the active role definition from `roles/*.md`
3. Context capture (in `context.py`):
   - If inside tmux, captures pane scrollback (commands and their output)
   - Otherwise, reads recent command history from `~/.zsh_history` or `~/.bash_history`
4. LLM setup (`llm.py`):
   - Loads API keys
   - Selects and configures the model and prepares the system prompt (including context note)
5. Command execution (`interaction.py`):
   - Each approved command runs independently via subprocess.run()
   - No state persistence between commands (cd, export, etc.)
6. Request-Response loop (`main.py`):
   - Prepares prompt for LLM (including system prompt, role, query, context)
   - Streams model responses in real time; prints the LLM's output
   - If the model suggests a command, prints it and prompts `[a]pprove / [r]eject / [m]odify`
   - Only runs commands if the user approves
   - Collects all command output, feeds it back to the conversation
   - Ends loop when there are no new commands or the user cancels

## Patterns and Constraints

- Command approval is required for every command (implemented in `interaction.py`)
- Commands execute independently using subprocess.run() with timeout support
- Two context types:
  - Deep (when in tmux): commands plus output
  - Shallow (history fallback): recent commands only
  - System prompt notes which type is present for best LLM results
- System context automatically includes OS, shell, and working directory information for platform-specific assistance
- No sudo/interactive password hacks; subprocess handles interactive commands normally, and authentication is always up to the user's TTY
- If required config or roles are missing or broken, whai intentionally crashes with a clear error

## Module Overview

- `main.py`: Conversation setup, core CLI, main execution loop, user IO
- `config.py`: Handles loading, creation, and validation of configuration and user roles, parses YAML in markdown files
- `context.py`: Implements context capture as described above
- `llm.py`: Manages LiteLLM backend, system prompt and API key setup, handles model streaming and tool use
- `interaction.py`: Handles command approval loop and executes approved commands via subprocess.run()
- `logging_setup.py`: Centralizes logging config, allowing debug toggles
- `tests/`: Contains unit and integration tests for all logic, using pytest

## Testing

- All modules have corresponding pytest test files in the `tests` directory
- Integration tests work with mock API or actual LLMs (user must provide keys and proper test markers)
- Tests never run dangerous commands; any such code is prevented by policy and design

## Configuration and Extensibility

- New LLMs can be added by extending `config.toml` and, if necessary, updating `llm.py`
   - Default model should (in the docs and code) should be gpt-5-mini, as it is fast, cheap and good.
- New roles are just markdown+YAML files in `~/.config/whai/roles/`
- To expand context types, extend `context.py`
- Extra shell helpers, plugins, or approval logic go in `interaction.py` or can be cleanly added elsewhere

## Troubleshooting

- The tool fails clearly and loudly if configuration or roles are missing/broken (fail-fast)
- Timeout handling for long-running commands is in `interaction.py`
- For debugging, use `whai_DEBUG=1` environment variable

## Summary

whai separates concerns in its source, requires user approval for every shell action, and executes commands independently without persistent state. All key logic is unit-tested and designed to be easy to extend or update.